# app.py

import streamlit as st
import openai
import faiss
import pickle
import numpy as np
from sentence_transformers import SentenceTransformer
from PyPDF2 import PdfReader
import tempfile
import os
import io
import json

from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload

# âœ… Streamlit ì„¤ì •
st.set_page_config(page_title="ğŸ“š PDF ê¸°ìˆ  í‚¤ì›Œë“œ ê²€ìƒ‰ê¸°", layout="wide")

# âœ… Google Drive ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
@st.cache_resource
def init_drive_service():
    credentials_info = st.secrets["google_credentials"]
    creds = service_account.Credentials.from_service_account_info(dict(credentials_info))
    service = build("drive", "v3", credentials=creds)
    return service

drive_service = init_drive_service()

# âœ… PDF ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜
def download_pdf_from_drive(service, file_id):
    request = service.files().get_media(fileId=file_id)
    fh = io.BytesIO()
    downloader = MediaIoBaseDownload(fh, request)
    done = False
    while not done:
        status, done = downloader.next_chunk()
    fh.seek(0)
    return fh

# âœ… ë²¡í„° ë¡œë”© í•¨ìˆ˜
@st.cache_resource
def load_vector_data(vector_dir):
    index = faiss.read_index(os.path.join(vector_dir, "faiss_index.index"))
    with open(os.path.join(vector_dir, "pages.pkl"), "rb") as f:
        pages = pickle.load(f)
    model = SentenceTransformer("all-mpnet-base-v2")
    return index, pages, model

# âœ… GPT í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜
def gpt_keywords(api_key, question):
    client = openai.OpenAI(api_key=api_key)
    prompt = f"""
ì§ˆë¬¸: "{question}"

ì´ ì§ˆë¬¸ê³¼ ì—°ê´€ëœ ê¸°ìˆ  í‚¤ì›Œë“œë¥¼ 10~15ê°œ ë½‘ì•„ì¤˜.
- í‚¤ì›Œë“œëŠ” ì§§ì€ ëª…ì‚¬í˜•ìœ¼ë¡œ (ì˜ˆ: ì˜¨ë””ë°”ì´ìŠ¤ AI, ëª¨ë¸ ì••ì¶•, ì—£ì§€ ì»´í“¨íŒ… ë“±)
- ì‰¼í‘œ(,)ë¡œ êµ¬ë¶„í•´ì„œ í•œ ì¤„ë¡œë§Œ ì¶œë ¥í•´ì¤˜.
"""
    res = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3,
        max_tokens=200
    )
    return [kw.strip() for kw in res.choices[0].message.content.split(",")]

# âœ… GPT ê²°ê³¼ ì •ë¦¬ í•¨ìˆ˜
def gpt_select_5(api_key, question, docs):
    client = openai.OpenAI(api_key=api_key)
    titles = "\n".join([
        f"- {doc['text'].splitlines()[0][:80]} (Page {doc['page']})"
        if doc['text'].strip() else f"- ì œëª© ì—†ìŒ (Page {doc['page']})"
        for doc in docs
    ])
    prompt = f"""
ì§ˆë¬¸: "{question}"

ì•„ë˜ëŠ” ê¸°ìˆ  ë¬¸ì„œë“¤ì˜ ì œëª©ê³¼ í˜ì´ì§€ ë²ˆí˜¸ì…ë‹ˆë‹¤.

{titles}

ì´ ì¤‘ì—ì„œ ì§ˆë¬¸ê³¼ ê´€ë ¨ë„ê°€ ë†’ì€ ê¸°ìˆ ì„ 5ê°œ ê³¨ë¼ ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì¤˜:

ì¶”ì²œê¸°ìˆ 1 : Page ë²ˆí˜¸ 
ê¸°ìˆ ì„¤ëª…1 : í•œ ì¤„ ì´ë‚´ë¡œ ê°„ë‹¨íˆ ìš”ì•½

ì¶”ì²œê¸°ìˆ 2 : ...
ê¸°ìˆ ì„¤ëª…2 : ...

í˜•ì‹ ê¼­ ë§ì¶°ì¤˜. ê²°ê³¼ëŠ” í•œêµ­ì–´ë¡œ ì •ë¦¬í•´ì¤˜.
"""
    res = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3,
        max_tokens=500
    )
    return res.choices[0].message.content

# âœ… FAISS ê²€ìƒ‰ í•¨ìˆ˜
def search_top_k(model, index, pages, keyword, k=4):
    query_vec = model.encode([keyword])
    D, I = index.search(np.array(query_vec), k)
    return [pages[i] for i in I[0]]

# âœ… UI êµ¬ì„±
st.title("ğŸ“š Google Drive ê¸°ë°˜ ê¸°ìˆ  ê²€ìƒ‰ê¸°")

api_key = st.text_input("ğŸ” OpenAI API Key", type="password")
question = st.text_input("ğŸ’¬ ê²€ìƒ‰í•  ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")

if api_key and question:
    pdf_file_id = st.secrets["pdf_file_id"]
    vector_dir = st.secrets["vector_dir"]

    with st.spinner("ğŸ“„ PDF íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘..."):
        pdf_data = download_pdf_from_drive(drive_service, pdf_file_id)
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
            tmp.write(pdf_data.read())
            tmp_path = tmp.name
        st.download_button("ğŸ“¥ PDF ë‹¤ìš´ë¡œë“œ", data=open(tmp_path, "rb").read(), file_name="document.pdf")
        st.subheader("ğŸ“„ PDF ë¯¸ë¦¬ë³´ê¸°")
        st.pdf(tmp_path)

    index, pages, model_embed = load_vector_data(vector_dir)

    with st.spinner("ğŸ” GPT í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë¬¸ì„œ ê²€ìƒ‰ ì¤‘..."):
        keywords = gpt_keywords(api_key, question)
        st.success("ğŸ“Œ ì¶”ì¶œëœ í‚¤ì›Œë“œ:")
        st.write(", ".join(keywords))

        all_docs = []
        seen = set()
        for kw in keywords:
            for doc in search_top_k(model_embed, index, pages, kw):
                key = (doc['page'], doc['text'][:50])
                if key not in seen:
                    all_docs.append(doc)
                    seen.add(key)

        result = gpt_select_5(api_key, question, all_docs)
        st.markdown("## âœ… GPT ì¶”ì²œ ê¸°ìˆ  5ê°œ")
        st.markdown(result)
